from __future__ import annotations

import hashlib
import io
import json
import zipfile
from dataclasses import asdict, dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional, Tuple

import numpy as np
import pandas as pd



# ============================================================
# STEP 1: UNIVERSAL LOADER (CSV/TSV/TXT/XLSX/XLS/ZIP)
# ============================================================

TEXT_EXTS = {".csv", ".tsv", ".txt"}
EXCEL_EXTS = {".xlsx", ".xls"}
ZIP_EXTS = {".zip"}

COMMON_ENCODINGS = ["utf-8", "utf-8-sig", "cp1252", "latin1"]


def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [
        str(c)
        .replace("\ufeff", "")  # remove BOM if present
        .strip()
        .lower()
        .replace("\n", " ")
        .replace("\r", " ")
        .replace("/", "_")      # platform/org -> platform_org
        .replace(" ", "_")
        for c in df.columns
    ]
    return df


def _try_read_text_table(data: bytes, sep: Optional[str] = None) -> pd.DataFrame:
    """
    Try reading bytes as delimited text using common encodings.
    sep=None => pandas will infer delimiter with engine="python"
    """
    last_err = None
    for enc in COMMON_ENCODINGS:
        try:
            s = data.decode(enc)
            bio = io.StringIO(s)
            df = pd.read_csv(
                bio,
                sep=sep,
                engine="python",
                dtype=str,       # keep as strings; coerce later
                na_filter=False, # treat empty as ""
            )
            if df.shape[1] >= 1:
                return df
        except Exception as e:
            last_err = e
    raise ValueError(f"Could not read as text table with common encodings. Last error: {last_err}")

def detect_header_row(df: pd.DataFrame, max_scan_rows: int = 10) -> int:
    """
    Detect likely header row by scoring rows based on:
    - number of non-empty cells
    - presence of typical header-like strings
    """
    header_keywords = {
        "amount", "revenue", "royalty", "earnings",
        "platform", "service", "territory", "country",
        "isrc", "track", "title", "period", "date",
        "units", "streams"
    }

    best_row = 0
    best_score = -1

    for i in range(min(max_scan_rows, len(df))):
        row = df.iloc[i].astype(str).str.lower()
        non_empty = row[row.str.strip() != ""]
        keyword_hits = sum(any(k in cell for k in header_keywords) for cell in row)

        score = len(non_empty) + (keyword_hits * 2)

        if score > best_score:
            best_score = score
            best_row = i

    return best_row

def _read_excel_all_sheets(excel_file: Any) -> Tuple[pd.DataFrame, List[str]]:
    """
    Read ALL sheets in an Excel workbook and concatenate into one DataFrame.
    Adds a 'sheet_name' column so we can trace each row back to its tab.
    Returns (df, sheet_names).
    """
    xls = pd.ExcelFile(excel_file)
    frames = []
    sheet_names = list(xls.sheet_names)

    for sheet in sheet_names:
        raw = pd.read_excel(xls, sheet_name=sheet, header=None, dtype=str, na_filter=False)
        header_row = detect_header_row(raw)
        df = pd.read_excel(xls, sheet_name=sheet, header=header_row, dtype=str, na_filter=False)
        if df is None or df.empty:
            continue
        df["sheet_name"] = sheet
        frames.append(df)

    if not frames:
        return pd.DataFrame(), sheet_names

    return pd.concat(frames, ignore_index=True), sheet_names

def _read_excel_as_units(excel_file: Any, source_meta: dict) -> List[dict]:
    """
    Read each sheet as a separate unit: [{"df":..., "meta":...}, ...]
    """
    xls = pd.ExcelFile(excel_file)
    units: List[dict] = []

    for sheet in xls.sheet_names:
        # read raw for header detection
        raw = pd.read_excel(xls, sheet_name=sheet, header=None, dtype=str, na_filter=False)
        header_row = detect_header_row(raw)

        df = pd.read_excel(xls, sheet_name=sheet, header=header_row, dtype=str, na_filter=False)
        if df is None or df.empty:
            continue

        df = _normalize_columns(df)
        df["_source_sheet"] = sheet

        meta = dict(source_meta)
        meta["loaded_as"] = "excel_sheet_unit"
        meta["sheet"] = sheet

        units.append({"df": df, "meta": meta})

    return units

def _read_excel_bytes_as_units(excel_bytes: bytes, source_meta: dict) -> List[dict]:
    """
    Read an in-memory Excel file (bytes) as separate sheet units.
    Forces an engine so pandas can open BytesIO.
    """
    bio = io.BytesIO(excel_bytes)

    zip_member = (source_meta or {}).get("zip_member", "")
    ext = Path(zip_member).suffix.lower()

    # xlsx => openpyxl, xls => xlrd (if installed) or fallback to openpyxl where possible
    if ext == ".xlsx":
        xls = pd.ExcelFile(bio, engine="openpyxl")
    elif ext == ".xls":
        # xlrd support depends on environment; try xlrd first, then openpyxl
        try:
            xls = pd.ExcelFile(bio, engine="xlrd")
        except Exception:
            xls = pd.ExcelFile(bio, engine="openpyxl")
    else:
        # default safest engine for modern Excel
        xls = pd.ExcelFile(bio, engine="openpyxl")

    # Now read sheets similarly to _read_excel_as_units, but using `xls` we already opened
    units: List[dict] = []
    for sheet in xls.sheet_names:
        raw = pd.read_excel(xls, sheet_name=sheet, header=None, dtype=str, na_filter=False)
        header_row = detect_header_row(raw)

        df = pd.read_excel(xls, sheet_name=sheet, header=header_row, dtype=str, na_filter=False)
        if df is None or df.empty:
            continue

        df = _normalize_columns(df)
        df["_source_sheet"] = sheet

        meta = dict(source_meta)
        meta["loaded_as"] = "zip_excel_sheet_unit"
        meta["sheet"] = sheet

        units.append({"df": df, "meta": meta})

    return units

def load_statement_file(path_str: str) -> Tuple[pd.DataFrame, dict]:
    """
    Load manager-style uploads into a DataFrame.
    Supports: CSV/TSV/TXT, XLSX/XLS, ZIP (containing supported files).
    Returns (df, metadata)
    """
    path = Path(path_str)
    if not path.exists():
        raise FileNotFoundError(f"File not found: {path}")

    ext = path.suffix.lower()
    meta = {"source_path": str(path), "source_type": ext}

    # Excel
    if ext in EXCEL_EXTS:
        df, sheet_names = _read_excel_all_sheets(path)
        df = _normalize_columns(df)

        # Ensure we have ONE consistent lineage column name
        # If your helper wrote "sheet_name", convert it to "_source_sheet"
        if "sheet_name" in df.columns and "_source_sheet" not in df.columns:
            df["_source_sheet"] = df["sheet_name"]
            df.drop(columns=["sheet_name"], inplace=True)

        meta["loaded_as"] = "excel_all_sheets"
        meta["sheets"] = sheet_names
        meta["sheet_count"] = len(sheet_names)

        return df, meta

    # Delimited text
    if ext in TEXT_EXTS:
        data = path.read_bytes()
        sep = "\t" if ext == ".tsv" else None
        df = _try_read_text_table(data, sep=sep)
        df = _normalize_columns(df)
        meta["loaded_as"] = "delimited_text"
        return df, meta

    # ZIP (choose first supported file inside)
    if ext in ZIP_EXTS:
        with zipfile.ZipFile(path, "r") as z:
            members = [m for m in z.namelist() if not m.endswith("/")]
            supported = [m for m in members if Path(m).suffix.lower() in (TEXT_EXTS | EXCEL_EXTS)]
            if not supported:
                raise ValueError("ZIP contained no supported files (.csv/.tsv/.txt/.xlsx/.xls).")

            # Prefer Excel over CSV/text
            supported.sort(key=lambda m: (Path(m).suffix.lower() not in EXCEL_EXTS, m))
            chosen = supported[0]
            meta["zip_member"] = chosen

            data = z.read(chosen)
            inner_ext = Path(chosen).suffix.lower()

            if inner_ext in EXCEL_EXTS:
                bio = io.BytesIO(data)
                xls = pd.ExcelFile(bio)
                dfs = []

                for sheet in xls.sheet_names:
                    df_sheet = pd.read_excel(xls, sheet_name=sheet, dtype=str, na_filter=False)
                    df_sheet["_source_sheet"] = sheet
                    dfs.append(df_sheet)

                df = pd.concat(dfs, ignore_index=True)
                df = _normalize_columns(df)

                meta["loaded_as"] = "zip_excel_all_sheets"
                meta["sheet"] = "ALL_SHEETS"

                return df, meta

            sep = "\t" if inner_ext == ".tsv" else None
            df = _try_read_text_table(data, sep=sep)
            df = _normalize_columns(df)
            meta["loaded_as"] = "zip_delimited_text"
            return df, meta

    raise ValueError(
        f"Unsupported file type: {ext}. Supported: {sorted(TEXT_EXTS | EXCEL_EXTS | ZIP_EXTS)}"
    )

def load_statement_package(path_str: str) -> Dict[str, Any]:
    """
    Loads a file into a package:
      {
        "package_meta": {...},
        "units": [{"df":..., "meta":...}, ...]
      }
    """
    path = Path(path_str)
    if not path.exists():
        raise FileNotFoundError(f"File not found: {path}")

    ext = path.suffix.lower()
    package_meta = {"source_path": str(path), "source_type": ext}
    units: List[dict] = []

    # Excel => one unit per sheet
    if ext in EXCEL_EXTS:
        source_meta = dict(package_meta)
        units = _read_excel_as_units(path, source_meta)
        package_meta["unit_count"] = len(units)
        package_meta["sheets"] = [u["meta"]["sheet"] for u in units]
        return {"package_meta": package_meta, "units": units}

    # Text => single unit
    if ext in TEXT_EXTS:
        data = path.read_bytes()
        sep = "\t" if ext == ".tsv" else None
        df = _try_read_text_table(data, sep=sep)
        df = _normalize_columns(df)
        units.append({"df": df, "meta": {**package_meta, "loaded_as": "delimited_text"}})
        package_meta["unit_count"] = 1
        return {"package_meta": package_meta, "units": units}

    # ZIP => Stage 3 (we’ll keep “first file” behavior for now)
    if ext in ZIP_EXTS:
        with zipfile.ZipFile(path, "r") as z:
            members = [m for m in z.namelist() if not m.endswith("/")]

            # ✅ FILTER OUT macOS ARTIFACTS
            members = [
                m for m in members
                if not m.startswith("__MACOSX/")
                   and "/__MACOSX/" not in m
                   and not Path(m).name.startswith("._")
            ]

            supported = [
                m for m in members
                if Path(m).suffix.lower() in (TEXT_EXTS | EXCEL_EXTS)
            ]

            if not supported:
                raise ValueError("ZIP contained no supported files (.csv/.tsv/.txt/.xlsx/.xls).")

            for member in supported:
                inner_ext = Path(member).suffix.lower()
                data = z.read(member)

                base_meta = dict(package_meta)
                base_meta["zip_member"] = member
                base_meta["source_type"] = inner_ext

                # ✅ Excel members
                if inner_ext in EXCEL_EXTS:
                    try:
                        excel_units = _read_excel_bytes_as_units(data, base_meta)
                        units.extend(excel_units)
                    except Exception as e:
                        package_meta.setdefault("skipped_members", []).append({
                            "zip_member": member,
                            "reason": f"{type(e).__name__}: {e}",
                        })
                    continue

                # ✅ Text members
                try:
                    sep = "\t" if inner_ext == ".tsv" else None
                    df = _try_read_text_table(data, sep=sep)
                    df = _normalize_columns(df)

                    meta = dict(base_meta)
                    meta["loaded_as"] = "zip_delimited_text_unit"
                    units.append({"df": df, "meta": meta})
                except Exception as e:
                    package_meta.setdefault("skipped_members", []).append({
                        "zip_member": member,
                        "reason": f"{type(e).__name__}: {e}",
                    })

        package_meta["unit_count"] = len(units)
        package_meta["zip_members_loaded"] = supported
        return {"package_meta": package_meta, "units": units}

    raise ValueError(f"Unsupported file type: {ext}")

# ============================================================
# STEP 2: FINDING SCHEMA + HELPERS
# ============================================================

@dataclass
class EvidenceRow:
    row_index: int
    fields: Dict[str, Any]


@dataclass
class Finding:
    id: str
    category: str
    severity: str          # "High" | "Med" | "Low"
    confidence: float      # 0.0 - 1.0
    summary: str
    recommended_action: str
    evidence: List[EvidenceRow]


def stable_id(*parts: str) -> str:
    raw = "||".join([p or "" for p in parts])
    return hashlib.sha256(raw.encode("utf-8")).hexdigest()[:12]


def coerce_numeric(series: pd.Series) -> pd.Series:
    return pd.to_numeric(
        series.astype(str)
        .str.replace(",", "", regex=False)
        .str.replace("$", "", regex=False),
        errors="coerce",
    )


def coerce_date(series: pd.Series) -> pd.Series:
    return pd.to_datetime(series, errors="coerce")


def ensure_period_month(df: pd.DataFrame) -> pd.DataFrame:
    """
    Ensures df has 'period_month' as YYYY-MM string for grouping.
    Prefers period_start; falls back to period_end.
    """
    df = df.copy()

    if "period_start" in df.columns and np.issubdtype(df["period_start"].dtype, np.datetime64):
        base = df["period_start"]
    elif "period_end" in df.columns and np.issubdtype(df["period_end"].dtype, np.datetime64):
        base = df["period_end"]
    else:
        df["period_month"] = None
        return df

    df["period_month"] = base.dt.to_period("M").astype(str)
    return df


def detect_amount_column(df: pd.DataFrame) -> str:
    """
    Detect an amount column in a manager export.
    Prefers name hints; falls back to "most numeric" column.
    """
    cols = list(df.columns)

    name_hints = [
        "revenue_usd", "amount_usd", "usd",
        "amount", "net_amount", "gross_amount",
        "royalty", "earnings", "payable",
        "revenue", "net_revenue", "gross_revenue",
        "total", "payment", "value",
    ]

    # exact match first
    for hint in name_hints:
        if hint in cols:
            return hint

    # substring match
    for c in cols:
        c_low = c.lower()
        if any(h in c_low for h in name_hints):
            return c

    # fallback: most numeric-like column
    best_col = None
    best_score = -1
    for c in cols:
        s = pd.to_numeric(
            df[c].astype(str).str.replace(",", "", regex=False).str.replace("$", "", regex=False),
            errors="coerce",
        )
        score = int(s.notna().sum())
        if score > best_score:
            best_score = score
            best_col = c

    if best_col is None or best_score < max(5, int(0.05 * len(df))):
        raise ValueError(
            f"Could not reliably detect an amount column. "
            f"Best numeric-like column was '{best_col}' with {best_score} numeric rows. "
            f"Columns found: {cols}"
        )

    return best_col


# ============================================================
# STEP 3: CHECKS
# ============================================================

def check_negative_amounts(df: pd.DataFrame, amount_col: str) -> List[Finding]:
    findings: List[Finding] = []
    if amount_col not in df.columns:
        return findings

    neg = df[df[amount_col] < 0]
    if len(neg) > 0:
        ev_cols = [c for c in ["period_start", "period_end", "platform_org", "territory", amount_col] if c in df.columns]
        ev = [EvidenceRow(int(i), {c: neg.loc[i, c] for c in ev_cols}) for i in neg.index.tolist()[:25]]

        findings.append(
            Finding(
                id=stable_id("negative_amounts", str(len(neg))),
                category="Negative amounts",
                severity="Med",
                confidence=0.85,
                summary=f"{len(neg)} rows have negative amounts (refunds/chargebacks/adjustments).",
                recommended_action="Confirm negatives are legitimate adjustments; if not, trace back to statement source and dispute with evidence.",
                evidence=ev,
            )
        )

    return findings


def check_duplicates_summary(df: pd.DataFrame, amount_col: str) -> List[Finding]:
    """
    Duplicates for summary statements: same platform_org + territory + period_start + period_end (+ amount)
    """
    findings: List[Finding] = []
    key_cols = [c for c in ["platform_org", "territory", "period_start", "period_end", amount_col] if c in df.columns]
    if len(key_cols) < 3:
        return findings

    dup_mask = df.duplicated(subset=key_cols, keep=False)
    dups = df[dup_mask]
    if len(dups) == 0:
        return findings

    ev = [EvidenceRow(int(i), {c: dups.loc[i, c] for c in key_cols}) for i in dups.index.tolist()[:25]]

    findings.append(
        Finding(
            id=stable_id("duplicates_summary", str(len(dups))),
            category="Possible duplicate rows",
            severity="Med" if len(dups) < 50 else "High",
            confidence=0.8,
            summary=f"{len(dups)} rows appear duplicated based on keys: {key_cols}.",
            recommended_action="Confirm whether duplicates are legitimate repeats (e.g., multiple sources). If not, dedupe and quantify impact.",
            evidence=ev,
        )
    )

    return findings


def check_suspicious_territories(df: pd.DataFrame) -> List[Finding]:
    findings: List[Finding] = []
    if "territory" not in df.columns:
        return findings

    terr = df["territory"].astype(str).str.strip()
    terr_l = terr.str.lower()

    suspicious = terr_l.isin({"", "unknown", "worldwide", "all", "global", "n/a", "na", "none"})
    idxs = df.index[suspicious].tolist()

    if idxs:
        ev = [EvidenceRow(int(i), {"territory": df.loc[i, "territory"]}) for i in idxs[:25]]
        findings.append(
            Finding(
                id=stable_id("suspicious_territories", str(len(idxs))),
                category="Unclear territory reporting",
                severity="Med",
                confidence=0.7,
                summary=f"{len(idxs)} rows use blank or catch-all territory values (e.g., Worldwide/Unknown).",
                recommended_action="Confirm whether revenue should be split by individual territories and whether any splits are missing.",
                evidence=ev,
            )
        )

    return findings


def check_missing_periods(df: pd.DataFrame) -> List[Finding]:
    """
    Flags gaps in period_start by (platform_org, territory).
    For monthly data, a gap > ~35 days is a good heuristic.
    """
    findings: List[Finding] = []
    required = {"platform_org", "territory", "period_start"}
    if not required.issubset(df.columns):
        return findings

    if not np.issubdtype(df["period_start"].dtype, np.datetime64):
        return findings

    grouped = df.dropna(subset=["period_start"]).groupby(["platform_org", "territory"])
    for (platform, territory), sub in grouped:
        periods = sub["period_start"].sort_values().unique()
        if len(periods) < 2:
            continue

        gaps: List[Tuple[pd.Timestamp, pd.Timestamp]] = []
        for i in range(1, len(periods)):
            delta_days = int((periods[i] - periods[i - 1]).days)
            if delta_days > 35:
                gaps.append((periods[i - 1], periods[i]))

        if gaps:
            ev = [
                EvidenceRow(
                    row_index=int(sub.index[0]),
                    fields={
                        "platform_org": platform,
                        "territory": territory,
                        "gap": f"{a.date()} → {b.date()}",
                    },
                )
                for a, b in gaps[:10]
            ]
            findings.append(
                Finding(
                    id=stable_id("missing_periods", str(platform), str(territory)),
                    category="Missing reporting periods",
                    severity="High",
                    confidence=0.8,
                    summary=f"{platform} / {territory} has {len(gaps)} gap(s) between reported periods.",
                    recommended_action="Request the missing statement(s) or confirm if reporting frequency changed for these period(s).",
                    evidence=ev,
                )
            )

    return findings


def check_platform_revenue_outliers(df: pd.DataFrame, amount_col: str) -> List[Finding]:
    """
    Outliers by platform_org using IQR. Works well for summary data.
    """
    findings: List[Finding] = []
    if "platform_org" not in df.columns or amount_col not in df.columns:
        return findings

    for platform, sub in df.groupby("platform_org"):
        vals = sub[amount_col].dropna()
        if len(vals) < 6:
            continue

        q1, q3 = np.percentile(vals, [25, 75])
        iqr = q3 - q1
        if iqr == 0:
            continue

        low, high = q1 - 3 * iqr, q3 + 3 * iqr
        out = sub[(sub[amount_col] < low) | (sub[amount_col] > high)]
        if len(out) == 0:
            continue

        ev_cols = [c for c in ["platform_org", "territory", "period_start", "period_end", amount_col] if c in df.columns]
        ev = [EvidenceRow(int(i), {c: out.loc[i, c] for c in ev_cols}) for i in out.index.tolist()[:25]]

        findings.append(
            Finding(
                id=stable_id("platform_outliers", str(platform)),
                category="Platform revenue anomalies",
                severity="Med",
                confidence=0.75,
                summary=f"{platform} shows unusually high/low revenue values compared to its own history.",
                recommended_action="Review whether rate changes, missing territory splits, or one-off adjustments explain these anomalies.",
                evidence=ev,
            )
        )

    return findings


def check_revenue_concentration(df: pd.DataFrame, amount_col: str) -> List[Finding]:
    findings: List[Finding] = []
    if amount_col not in df.columns:
        return findings

    total = float(df[amount_col].fillna(0).sum())
    if total <= 0:
        return findings

    def _severity_for(share: float, total_amt: float) -> Tuple[str, float]:
        severity = "Low"
        confidence = 0.75

        if share >= 0.95:
            if total_amt >= 1_000_000:
                return "High", 0.9
            if total_amt >= 100_000:
                return "Med", 0.85
            return "Low", 0.8

        if share >= 0.80:
            if total_amt >= 1_000_000:
                return "Med", 0.85
            if total_amt >= 250_000:
                return "Med", 0.8
            return "Low", 0.75

        return severity, confidence

    # Platform concentration
    if "platform_org" in df.columns:
        plat = (
            df.assign(_amt=df[amount_col].fillna(0))
              .groupby("platform_org")["_amt"]
              .sum()
              .sort_values(ascending=False)
        )
        if not plat.empty:
            top_plat = plat.head(5)
            top_share = float(top_plat.iloc[0]) / total if total else 0.0
            severity, conf = _severity_for(top_share, total)

            ev = [EvidenceRow(row_index=0, fields={"platform_org": k, "revenue": float(v)}) for k, v in top_plat.items()]

            summary = (
                f"All reported revenue comes from {top_plat.index[0]}."
                if len(top_plat) == 1
                else f"Top platform contributes {top_share:.0%} of total revenue (top 5 shown)."
            )

            findings.append(
                Finding(
                    id=stable_id("revenue_concentration_platform"),
                    category="Revenue concentration",
                    severity=severity,
                    confidence=conf,
                    summary=summary,
                    recommended_action=(
                        "Use this to sanity-check expectations and prioritize deeper audits on the largest revenue sources. "
                        "If this concentration is unexpected, confirm whether additional platform statements exist or whether the export was filtered."
                    ),
                    evidence=ev,
                )
            )

    # Territory concentration
    if "territory" in df.columns:
        terr = (
            df.assign(_amt=df[amount_col].fillna(0))
              .groupby("territory")["_amt"]
              .sum()
              .sort_values(ascending=False)
        )
        if not terr.empty:
            top_terr = terr.head(5)
            top_share = float(top_terr.iloc[0]) / total if total else 0.0
            severity, conf = _severity_for(top_share, total)

            ev = [EvidenceRow(row_index=0, fields={"territory": k, "revenue": float(v)}) for k, v in top_terr.items()]

            summary = (
                f"All reported revenue comes from {top_terr.index[0]}."
                if len(top_terr) == 1
                else f"Top territory contributes {top_share:.0%} of total revenue (top 5 shown)."
            )

            findings.append(
                Finding(
                    id=stable_id("revenue_concentration_territory"),
                    category="Revenue concentration",
                    severity=severity,
                    confidence=conf,
                    summary=summary,
                    recommended_action=(
                        "Use this to verify major markets are present and consistent period-over-period. "
                        "If this concentration is unexpected, confirm whether global territory splits or additional exports exist."
                    ),
                    evidence=ev,
                )
            )

    return findings


# ============================================================
# STATEMENT TYPE + STANDARD SCHEMA
# ============================================================

StatementType = Literal["summary", "line_item", "ambiguous"]


def _colset(df: pd.DataFrame) -> set[str]:
    return set([c.lower().strip() for c in df.columns])


def detect_statement_type(df: pd.DataFrame) -> StatementType:
    cols = _colset(df)
    nrows = len(df)

    line_item_signals = {
        "isrc", "upc", "track_title", "track", "track_name", "song", "song_title", "title",
        "album", "artist", "composer", "writer",
        "streams", "stream_count", "plays", "play_count", "units", "quantity",
        "rate", "per_stream_rate", "unit_rate", "payout_rate",
        "content_id", "video_id", "asset_id",
    }

    summary_signals = {
        "platform_org", "platform", "service", "store",
        "territory", "country", "region",
        "period", "period_start", "period_end", "statement_date",
        "revenue_usd", "amount", "net_amount", "earnings", "royalty", "total",
    }

    li_hits = len(cols.intersection(line_item_signals))
    sum_hits = len(cols.intersection(summary_signals))

    if li_hits >= 2:
        return "line_item"

    if nrows <= 25 and sum_hits >= 3 and li_hits == 0:
        return "summary"

    if nrows >= 200 and li_hits >= 1:
        return "line_item"

    return "ambiguous"

StatementFamily = Literal[
    "DISTRIBUTOR_LABEL_MASTER",
    "MLC_MECHANICAL",
    "PRO_PERFORMANCE",
    "NEIGHBORING_RIGHTS",
    "UGC_CONTENT_ID",
    "PUBLISHING_ADMIN",
    "UNKNOWN",
]

def classify_statement_family(df: pd.DataFrame, meta: dict) -> Dict[str, Any]:
    """
    Heuristic classifier using:
    - zip_member / sheet naming hints
    - column signals
    - source_type / platform_org signals
    Returns {"family": ..., "confidence": 0..1, "reasons": [...]}
    """
    cols = set([c.lower() for c in df.columns])
    reasons: List[str] = []

    # ---- Name-based hints (very strong in real packages) ----
    name_parts = []
    if meta:
        for k in ["zip_member", "sheet", "source_path"]:
            v = meta.get(k)
            if v:
                name_parts.append(str(v).lower())
    name_blob = " ".join(name_parts)

    def has_name(*terms: str) -> bool:
        return any(t.lower() in name_blob for t in terms)

    # MLC
    if has_name("mlc", "mechanical licensing collective"):
        return {"family": "MLC_MECHANICAL", "confidence": 0.9, "reasons": ["Name hint: MLC/mechanical"]}

    # PRO
    if has_name("ascap", "bmi", "sesac", "prs", "sacem", "gema", "apra", "socan", "pro"):
        return {"family": "PRO_PERFORMANCE", "confidence": 0.9, "reasons": ["Name hint: PRO society"]}

    # Neighboring rights
    if has_name("soundexchange", "ppl", "gvl", "sppf", "neighboring", "neighbouring"):
        return {"family": "NEIGHBORING_RIGHTS", "confidence": 0.9, "reasons": ["Name hint: neighboring rights society"]}

    # UGC / Content ID
    if has_name("content id", "contentid", "ugc", "youtube", "meta", "facebook", "instagram", "tiktok"):
        # Still check columns too, but name is strong enough to label
        reasons.append("Name hint: UGC/Content ID platforms")

    # Publishing admin
    if has_name("publishing", "publisher", "admin", "subpub", "royaltyshare", "songtrust", "sentric", "kobalt"):
        reasons.append("Name hint: publishing admin")

    # ---- Column-based signals ----
    # MLC mechanical tends to include "work" identifiers: iswc, writer, publisher, share, mechanical
    mlc_signals = {"iswc", "work_title", "writer", "publisher", "mechanical", "usage_type", "mechanicals"}
    pro_signals = {"performance", "cue", "program", "episode", "pro", "broadcast", "station", "venue"}
    nr_signals = {"neighboring", "neighbouring", "soundexchange", "ppl", "featured_artist", "nonfeatured", "performer"}
    ugc_signals = {"content_id", "asset_id", "video_id", "channel", "claim", "monetized_playbacks"}
    dist_signals = {"platform_org", "dsp", "store", "service", "spotify", "apple", "amazon", "youtube_music", "tidal"}

    # Helper: count signal hits
    def hit_count(sig: set[str]) -> int:
        return len(cols.intersection(sig))

    mlc_hits = hit_count(mlc_signals)
    pro_hits = hit_count(pro_signals)
    nr_hits = hit_count(nr_signals)
    ugc_hits = hit_count(ugc_signals)
    dist_hits = hit_count(dist_signals)

    # UGC override if explicit IDs present
    if ugc_hits >= 1 and ("asset_id" in cols or "content_id" in cols or "video_id" in cols):
        return {"family": "UGC_CONTENT_ID", "confidence": 0.75, "reasons": reasons + ["Columns: Content ID / asset signals"]}

    # MLC
    if mlc_hits >= 2:
        return {"family": "MLC_MECHANICAL", "confidence": 0.75, "reasons": reasons + [f"Columns: mechanical/work signals ({mlc_hits})"]}

    # PRO
    if pro_hits >= 2:
        return {"family": "PRO_PERFORMANCE", "confidence": 0.7, "reasons": reasons + [f"Columns: performance signals ({pro_hits})"]}

    # Neighboring rights
    if nr_hits >= 2:
        return {"family": "NEIGHBORING_RIGHTS", "confidence": 0.7, "reasons": reasons + [f"Columns: neighboring rights signals ({nr_hits})"]}

    # Publishing admin (weaker; often looks like line_item but not DSP)
    if ("publisher" in cols or "writer" in cols) and dist_hits == 0:
        return {"family": "PUBLISHING_ADMIN", "confidence": 0.6, "reasons": reasons + ["Columns: publisher/writer with no DSP signals"]}

    # Distributor/Label: DSP/service/platform present OR your normalized schema is clearly DSP-based
    # Distributor/Label (master) — broader summary fallback
    summary_like = (
            ("amount" in cols) and
            (("period_start" in cols) or ("period_end" in cols) or ("period" in cols)) and
            ("territory" in cols)
    )

    if ("platform_org" in cols) or (dist_hits >= 1):
        return {"family": "DISTRIBUTOR_LABEL_MASTER", "confidence": 0.65,
                "reasons": reasons + ["Columns: DSP/platform signals"]}

    # ✅ NEW: if it's summary-like and has common distributor statement fields, classify as distributor/label
    if summary_like and ("source_type" in cols or "tier" in cols):
        return {
            "family": "DISTRIBUTOR_LABEL_MASTER",
            "confidence": 0.6,
            "reasons": reasons + ["Columns: summary-like statement (amount/period/territory)"]
        }

    # Name-hint fallback for UGC/publishing if flagged earlier
    if any("ugc/content" in r.lower() for r in reasons):
        return {"family": "UGC_CONTENT_ID", "confidence": 0.55, "reasons": reasons}

    if any("publishing" in r.lower() for r in reasons):
        return {"family": "PUBLISHING_ADMIN", "confidence": 0.55, "reasons": reasons}

    return {"family": "UNKNOWN", "confidence": 0.4, "reasons": reasons or ["Insufficient signals"]}

def standardize_schema(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    cols = list(df.columns)

    mapping = {
        # Platform
        "platform": "platform_org",
        "service": "platform_org",
        "store": "platform_org",
        "dsp": "platform_org",
        "partner": "platform_org",

        # Territory
        "country": "territory",
        "region": "territory",
        "market": "territory",
        "territories": "territory",

        # Period
        "start_date": "period_start",
        "periodstart": "period_start",
        "from": "period_start",
        "end_date": "period_end",
        "periodend": "period_end",
        "to": "period_end",
        "statement_date": "statement_issue_date",

        # Amount / earnings
        "revenue": "amount",
        "revenue_usd": "amount",
        "net_revenue": "amount",
        "gross_revenue": "amount",
        "earnings": "amount",
        "royalty": "amount",
        "payable": "amount",
        "net_amount": "amount",
        "total": "amount",
        "amount_usd": "amount",
        "usd": "amount",

        # Identifiers
        "track": "track_title",
        "track_name": "track_title",
        "song": "track_title",
        "song_title": "track_title",
        "title": "track_title",
        "isrc_code": "isrc",
        "recording_isrc": "isrc",
        "upc_code": "upc",

        # Quantity
        "streams": "quantity",
        "stream_count": "quantity",
        "plays": "quantity",
        "play_count": "quantity",
        "units": "quantity",
        "unit_count": "quantity",
        "qty": "quantity",

        # Rate
        "payout_rate": "rate",
        "per_stream_rate": "rate",
        "unit_rate": "rate",

        # Currency
        "ccy": "currency",
        "curr": "currency",
    }

    rename = {}
    for c in cols:
        c_low = c.lower().strip()
        if c_low in mapping:
            rename[c] = mapping[c_low]

    if rename:
        df = df.rename(columns=rename)

    return df


def check_zero_or_blank_amounts(df: pd.DataFrame, amount_col: str) -> List[Finding]:
    findings: List[Finding] = []
    if amount_col not in df.columns:
        return findings

    amt = df[amount_col].fillna(0)
    mask = (amt == 0) | (~np.isfinite(amt))
    bad = df[mask]

    if len(bad) > 0:
        ev_cols = [c for c in ["platform_org", "territory", "period_start", "period_end", "track_title", "isrc", amount_col] if c in df.columns]
        ev = [EvidenceRow(int(i), {c: bad.loc[i, c] for c in ev_cols}) for i in bad.index.tolist()[:25]]
        findings.append(
            Finding(
                id=stable_id("zero_amounts", str(len(bad))),
                category="Zero/blank earnings lines",
                severity="Low" if len(bad) < 50 else "Med",
                confidence=0.7,
                summary=f"{len(bad)} rows have zero/blank earnings. This may be normal (promos/rounding) or may indicate missing rates or incomplete reporting.",
                recommended_action="Spot-check a sample: confirm these rows have expected usage/quantity and correct rate/earnings calculations.",
                evidence=ev,
            )
        )
    return findings


def check_missing_line_item_ids(df: pd.DataFrame) -> List[Finding]:
    findings: List[Finding] = []
    id_cols = [c for c in ["isrc", "track_title", "upc"] if c in df.columns]
    if not id_cols:
        return findings

    mask = df[id_cols].isna() | (df[id_cols].astype(str).apply(lambda col: col.str.strip() == ""))
    all_missing = mask.all(axis=1)
    idxs = df.index[all_missing].tolist()

    if idxs:
        ev = [EvidenceRow(int(i), {c: df.loc[i, c] for c in id_cols}) for i in idxs[:25]]
        findings.append(
            Finding(
                id=stable_id("missing_line_item_ids", str(len(idxs))),
                category="Missing track identifiers",
                severity="Med" if len(idxs) < 100 else "High",
                confidence=0.85,
                summary=f"{len(idxs)} line-item rows are missing key identifiers (ISRC/title/UPC), limiting matching and recovery workflows.",
                recommended_action="Fill identifiers where possible (prefer ISRC). If unavailable, map titles to a canonical catalog list.",
                evidence=ev,
            )
        )
    return findings


def check_rate_math(df: pd.DataFrame, amount_col: str) -> List[Finding]:
    findings: List[Finding] = []
    if amount_col not in df.columns or "quantity" not in df.columns or "rate" not in df.columns:
        return findings

    qty = coerce_numeric(df["quantity"])
    rate = coerce_numeric(df["rate"])
    amt = df[amount_col]

    expected = qty * rate
    mask = expected.notna() & amt.notna() & qty.notna() & rate.notna()
    if mask.sum() < 10:
        return findings

    denom = expected.abs().replace(0, np.nan)
    rel_err = (amt - expected).abs() / denom
    bad_mask = rel_err > 0.05

    bad_idxs = df.index[mask & bad_mask].tolist()
    if bad_idxs:
        ev_cols = [c for c in ["platform_org", "territory", "period_start", "track_title", "isrc", "quantity", "rate", amount_col] if c in df.columns]
        ev = [EvidenceRow(int(i), {c: df.loc[i, c] for c in ev_cols}) for i in bad_idxs[:25]]
        findings.append(
            Finding(
                id=stable_id("rate_math_mismatch", str(len(bad_idxs))),
                category="Rate × quantity mismatches",
                severity="Med",
                confidence=0.75,
                summary=f"{len(bad_idxs)} rows deviate from earnings ≈ quantity × rate beyond tolerance.",
                recommended_action="Verify whether amounts include fees/adjustments, currency conversion, or whether rate/quantity columns represent different units.",
                evidence=ev,
            )
        )
    return findings


def build_royalty_stream_coverage(df: pd.DataFrame, statement_type: str) -> List[Dict[str, Any]]:
    cols = set(df.columns)
    has_platform = "platform_org" in cols
    has_amount = "amount" in cols
    has_territory = "territory" in cols

    master_streaming_present = bool(has_amount and has_platform)

    coverage = [
        {
            "stream": "Interactive Streaming (Master)",
            "represented_in_upload": True if master_streaming_present else "Unknown",
            "usually_reported_by": "Distributor / Label",
            "notes": "Typically includes DSPs like Spotify/Apple/Amazon/YouTube Music depending on distribution footprint.",
        },
        {
            "stream": "Digital Downloads (Master)",
            "represented_in_upload": False,
            "usually_reported_by": "Distributor / Label",
            "notes": "Often separate line items; may not appear if no download sales occurred.",
        },
        {
            "stream": "Physical Sales (Master)",
            "represented_in_upload": False,
            "usually_reported_by": "Distributor / Label",
            "notes": "Applicable only if physical products are sold (CD/vinyl).",
        },
        {
            "stream": "Non-interactive Digital Performance (Master)",
            "represented_in_upload": False,
            "usually_reported_by": "SoundExchange (US) / digital radio services",
            "notes": "Typically reported outside distributor statements; timing varies.",
        },
        {
            "stream": "International Neighboring Rights (Master)",
            "represented_in_upload": False,
            "usually_reported_by": "Foreign neighboring rights societies (PPL, GVL, etc.)",
            "notes": "Territory-dependent; often delayed; requires correct registrations.",
        },
        {
            "stream": "Mechanical Royalties (Composition)",
            "represented_in_upload": False,
            "usually_reported_by": "MLC (US) / mechanical societies / publisher",
            "notes": "Commonly generated by streaming/downloads; often not included in distributor revenue exports.",
        },
        {
            "stream": "Performance Royalties (Composition)",
            "represented_in_upload": False,
            "usually_reported_by": "PROs (ASCAP/BMI/SESAC, PRS, SACEM, etc.)",
            "notes": "Reported separately; territory-dependent; frequently delayed vs. streaming statements.",
        },
        {
            "stream": "Synchronization (Sync Licensing)",
            "represented_in_upload": "Unknown",
            "usually_reported_by": "Publisher / licensing entity",
            "notes": "Only applicable if placements exist (film/TV/ads/games).",
        },
        {
            "stream": "UGC / Content ID Monetization",
            "represented_in_upload": "Unknown",
            "usually_reported_by": "YouTube/Meta CMS or distributor/CMS partner",
            "notes": "Depends on rights setup and claims; often separate from DSP streaming exports.",
        },
    ]

    if statement_type == "summary" and master_streaming_present and has_territory:
        for item in coverage:
            if item["stream"] == "Interactive Streaming (Master)":
                item["notes"] += " This upload looks like a summary statement (aggregated)."

    return coverage


def build_contextual_range_estimates(total_streaming_amount: float) -> List[Dict[str, Any]]:
    base = float(total_streaming_amount or 0.0)
    if base <= 0:
        return []

    def rng(lo: float, hi: float) -> Dict[str, float]:
        return {"low": round(base * lo, 2), "high": round(base * hi, 2)}

    return [
        {
            "stream": "Mechanical Royalties (Composition)",
            "typical_percent_of_streaming": "8%–15%",
            "estimated_range_usd": rng(0.08, 0.15),
            "confidence": "Medium",
            "methodology_note": "Contextual range based on typical mechanical outcomes for interactive streaming; varies by songwriter share, splits, registrations, and territory.",
        },
        {
            "stream": "Performance Royalties (Composition)",
            "typical_percent_of_streaming": "3%–8%",
            "estimated_range_usd": rng(0.03, 0.08),
            "confidence": "Medium",
            "methodology_note": "Contextual range based on typical PRO distributions for comparable catalogs; reporting timelines vary and may be delayed.",
        },
        {
            "stream": "Non-interactive Digital Performance (Master)",
            "typical_percent_of_streaming": "2%–5%",
            "estimated_range_usd": rng(0.02, 0.05),
            "confidence": "Low–Medium",
            "methodology_note": "Applies if non-interactive digital radio usage exists (e.g., Pandora radio, SiriusXM, web radio); often reported separately.",
        },
        {
            "stream": "International Neighboring Rights (Master)",
            "typical_percent_of_streaming": "1%–4%",
            "estimated_range_usd": rng(0.01, 0.04),
            "confidence": "Low",
            "methodology_note": "Highly dependent on territory, repertoire, and registrations; often delayed and collected via foreign societies.",
        },
    ]


def build_prioritized_opportunities(estimated_ranges: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    if not estimated_ranges:
        return []

    confidence_weight = {
        "High": 1.0,
        "Medium": 0.75,
        "Low–Medium": 0.6,
        "Low": 0.4,
    }

    ranked = []
    for item in estimated_ranges:
        low = item["estimated_range_usd"]["low"]
        high = item["estimated_range_usd"]["high"]
        midpoint = (low + high) / 2

        conf = item.get("confidence", "Low")
        weight = confidence_weight.get(conf, 0.5)
        score = midpoint * weight

        ranked.append({
            "stream": item["stream"],
            "estimated_midpoint_usd": round(midpoint, 2),
            "confidence": conf,
            "priority_score": round(score, 2),
        })

    ranked.sort(key=lambda x: x["priority_score"], reverse=True)
    for i, r in enumerate(ranked, start=1):
        r["priority_rank"] = i

    return ranked


def check_missing_expected_platforms(df: pd.DataFrame, amount_col: str) -> List[Finding]:
    findings: List[Finding] = []
    if "platform_org" not in df.columns or amount_col not in df.columns:
        return findings

    total = float(df[amount_col].fillna(0).sum())
    if total <= 0:
        return findings

    platforms = (
        df["platform_org"].astype(str).str.strip().replace({"": np.nan}).dropna()
    )

    unique_platforms = sorted(set(platforms.tolist()))
    if len(unique_platforms) != 1:
        return findings

    only_platform = unique_platforms[0]

    if total >= 1_000_000:
        severity = "High"
        confidence = 0.9
    elif total >= 100_000:
        severity = "Med"
        confidence = 0.85
    else:
        severity = "Low"
        confidence = 0.75

    findings.append(
        Finding(
            id=stable_id("missing_expected_platforms", only_platform),
            category="Coverage gap risk",
            severity=severity,
            confidence=confidence,
            summary=(
                f"Revenue is concentrated in a single platform ({only_platform}) "
                f"representing ${total:,.2f}. "
                "If multi-platform distribution exists, confirm whether additional statements are missing."
            ),
            recommended_action=(
                "Confirm coverage across Apple Music, Amazon Music, YouTube Music, Pandora, "
                "and other DSPs where applicable. Upload additional statements to expand scope."
            ),
            evidence=[EvidenceRow(row_index=0, fields={"platform_org": only_platform, "total_reported": round(total, 2)})],
        )
    )

    if "territory" in df.columns:
        territories = df["territory"].astype(str).str.strip().replace({"": np.nan}).dropna()
        unique_territories = sorted(set(territories.tolist()))
        if len(unique_territories) == 1:
            only_territory = unique_territories[0]

            if total >= 1_000_000:
                terr_severity, terr_confidence = "High", 0.85
            elif total >= 100_000:
                terr_severity, terr_confidence = "Med", 0.8
            else:
                terr_severity, terr_confidence = "Low", 0.7

            findings.append(
                Finding(
                    id=stable_id("single_territory_export", only_platform, only_territory),
                    category="Territory concentration risk",
                    severity=terr_severity,
                    confidence=terr_confidence,
                    summary=(
                        f"Revenue is reported from a single territory ({only_territory}). "
                        "If international activity is expected, confirm whether global reporting is included."
                    ),
                    recommended_action=(
                        "Verify whether additional territory splits or global exports are available "
                        "for the same reporting period."
                    ),
                    evidence=[EvidenceRow(row_index=0, fields={"territory": only_territory})],
                )
            )

    return findings


def build_top_revenue_drivers(df: pd.DataFrame, amount_col: str, top_n: int = 10) -> List[Dict[str, Any]]:
    if amount_col not in df.columns:
        return []

    key_cols = []
    if "isrc" in df.columns:
        key_cols.append("isrc")
    if "track_title" in df.columns:
        key_cols.append("track_title")
    if not key_cols:
        return []

    work_col = "isrc" if "isrc" in df.columns else "track_title"

    tmp = df.copy()
    tmp["_amt"] = tmp[amount_col].fillna(0)

    group_cols = [work_col]
    if "platform_org" in tmp.columns:
        group_cols.append("platform_org")
    if "territory" in tmp.columns:
        group_cols.append("territory")

    agg = (
        tmp.groupby(group_cols, dropna=False)["_amt"]
        .sum()
        .reset_index()
        .sort_values("_amt", ascending=False)
        .head(top_n)
    )

    results: List[Dict[str, Any]] = []
    for _, row in agg.iterrows():
        work_id = str(row.get(work_col, ""))

        item = {
            "work_id": work_id,
            "track_title": str(row.get("track_title", work_id)),
            "platform_org": str(row.get("platform_org", "")),
            "territory": str(row.get("territory", "")),
            "amount_usd": float(row["_amt"]),
        }

        if "track_title" in df.columns and work_col != "track_title":
            isrc_val = row.get("isrc")
            if isrc_val is not None:
                titles = df[df["isrc"] == isrc_val]["track_title"].astype(str).str.strip()
                if len(titles) > 0:
                    item["track_title"] = titles.value_counts().index[0]

        results.append(item)

    return results


# ============================================================
# TREND INTELLIGENCE (CROSS-PERIOD)
# ============================================================

@dataclass
class TrendSignal:
    signal_type: str
    severity_score: float
    confidence: float
    direction: str
    summary: str
    recommended_action: str
    evidence: Dict[str, Any]


def _clip01(x: float) -> float:
    return float(max(0.0, min(1.0, x)))

def severity_label(score: float) -> str:
    """
    Converts 0.0–1.0 score into user-friendly severity buckets.
    """
    s = float(score or 0.0)
    if s >= 0.75:
        return "High"
    if s >= 0.40:
        return "Med"
    return "Low"


def confidence_label(score: float) -> str:
    """
    Converts 0.0–1.0 into user-friendly confidence buckets.
    """
    c = float(score or 0.0)
    if c >= 0.85:
        return "High"
    if c >= 0.65:
        return "Med"
    return "Low"



def _linear_slope(y: np.ndarray) -> float:
    n = len(y)
    if n < 2:
        return 0.0
    x = np.arange(n, dtype=float)
    x_mean = x.mean()
    y_mean = float(np.mean(y))
    num = float(np.sum((x - x_mean) * (y - y_mean)))
    den = float(np.sum((x - x_mean) ** 2))
    return 0.0 if den == 0 else num / den


def classify_behavior(series: np.ndarray) -> Dict[str, Any]:
    """
    Classify a revenue series into simple behavior buckets using:
    - normalized slope (trend)
    - volatility (coefficient of variation)
    """
    s = np.array(series, dtype=float)
    s = s[np.isfinite(s)]

    if len(s) < 4:
        return {
            "label": "insufficient_data",
            "slope_normalized": 0.0,
            "volatility_cv": 0.0,
        }

    mean = float(np.mean(s))
    mean = mean if mean != 0 else 1e-9

    slope = _linear_slope(s)  # uses your existing helper
    slope_norm = float(slope / mean)

    std = float(np.std(s))
    cv = float(std / mean) if mean > 0 else 0.0

    # Simple rules
    if cv >= 0.35:
        label = "volatile"
    elif slope_norm >= 0.05:
        label = "increasing"
    elif slope_norm <= -0.05:
        label = "declining"
    else:
        label = "stable"

    return {
        "label": label,
        "slope_normalized": slope_norm,
        "volatility_cv": cv,
    }


def compute_revenue_risk_level(trend: Dict[str, Any] | None) -> Dict[str, Any]:
    trend = trend or {}
    signals = trend.get("signals", []) or []
    """
    Converts trend_intelligence signals into a single user-friendly takeaway.
    """
    signals = trend.get("signals", []) or []

    drivers: List[str] = []
    level = "Low"

    # If any behavior/trajectory says declining OR severity is high, escalate
    for s in signals:
        st = (s.get("signal_type") or "").strip()
        direction = (s.get("direction") or "").strip().lower()
        sev = (s.get("severity") or "").strip()

        if st == "REVENUE_TRAJECTORY" and direction == "decreasing":
            drivers.append("Revenue trending downward")
            level = "Moderate" if level == "Low" else level

        if st == "PORTFOLIO_BEHAVIOR" and direction == "volatile":
            drivers.append("Unstable revenue behavior (volatility)")
            level = "Moderate" if level == "Low" else level

        # If we ever label something High severity, bump overall level
        if sev == "High":
            level = "High"

    # Meaning text (what a manager understands instantly)
    meaning_map = {
        "Low": (
            "Revenue looks relatively stable across the months analyzed. "
            "No urgent signals detected, but you can still investigate small gaps if desired."
        ),
        "Moderate": (
            "Revenue behavior shows instability or early decline signals. "
            "Investigation is recommended, but no critical structural failures were detected."
        ),
        "High": (
            "Revenue signals suggest a material decline or high-risk instability. "
            "Prioritize investigation immediately and validate completeness of reporting coverage."
        ),
    }

    # Action text (where to start)
    action_map = {
        "Low": (
            "Confirm registrations are complete (MLC/PROs) and ensure all expected platforms are represented. "
            "If desired, use Top Revenue Drivers to prioritize smaller recovery opportunities."
        ),
        "Moderate": (
            "Check whether any months, platforms, or territories are missing, then verify platform mix changes "
            "and adjustments. Request line-item exports if you only have summary-level data."
        ),
        "High": (
            "Validate missing periods, confirm platform coverage, and reconcile rate or usage changes. "
            "Immediately request the most detailed line-item export available."
        ),
    }

    # De-dupe driver list but keep order
    drivers = list(dict.fromkeys([d for d in drivers if d]))

    return {
        "revenue_risk_level": level,
        "meaning": meaning_map[level],
        "key_drivers": drivers,
        "recommended_start": action_map[level],
    }



def compute_trend_intelligence(df: pd.DataFrame, amount_col: str) -> Dict[str, Any]:
    out: Dict[str, Any] = {
        "window_months": 0,
        "signals": [],
        "platform_behavior": [],
        "territory_behavior": [],
    }

    if "period_month" not in df.columns or df["period_month"].isna().all():
        return out

    d = df.dropna(subset=["period_month"]).copy()
    if d.empty:
        return out

    monthly = (
        d.groupby("period_month")[amount_col]
        .sum()
        .reset_index()
        .sort_values("period_month")
    )

    monthly = monthly.rename(columns={amount_col: "revenue"})
    out["window_months"] = int(len(monthly))
    monthly["period_month"] = monthly["period_month"].astype(str)  # JSON-safe + consistent labels

    months_list = monthly["period_month"].tolist()

    # Keep your existing range object (good for the report)
    out["period_range"] = {
        "start": months_list[0] if months_list else None,
        "end": months_list[-1] if months_list else None,
    }

    # Add a human-friendly window label (best for executive text)
    out["window_start_month"] = out["period_range"]["start"]
    out["window_end_month"] = out["period_range"]["end"]
    out["window_label"] = (
        f"{out['window_start_month']} → {out['window_end_month']}"
        if out["window_start_month"] and out["window_end_month"]
        else None
    )

    # Keep your monthly detail list (great for showing evidence)
    out["monthly_revenue"] = [
        {"period_month": str(m), "revenue": float(r)}
        for m, r in zip(monthly["period_month"], monthly["revenue"])
    ]

    if len(monthly) < 4:
        return out

    # ---------- Portfolio behavior ----------
    portfolio_series = monthly["revenue"].to_numpy(dtype=float)
    out["portfolio_behavior"] = classify_behavior(portfolio_series)

    # Also emit a signal if it's declining or volatile
    pb = out["portfolio_behavior"] or {}
    pb_label = pb.get("label")
    if pb_label in {"declining", "volatile"}:
        sev_score = 0.85 if pb_label == "declining" else 0.70
        conf_score = 0.90

        out["signals"].append({
            "signal_type": "PORTFOLIO_BEHAVIOR",
            "severity": severity_label(sev_score),
            "confidence": confidence_label(conf_score),
            "direction": pb_label,
            "summary": (
                "Portfolio revenue is declining across the reporting window."
                if pb_label == "declining"
                else "Portfolio revenue is volatile across the reporting window."
            ),
            "recommended_action": (
                "Investigate whether this is driven by missing months, platform mix shifts, "
                "territory changes, or rate/usage issues."
            ),
            "evidence": pb,
        })

    # ---------- Portfolio revenue trajectory (your existing slope logic) ----------
    N = min(6, len(monthly))
    slope = _linear_slope(monthly["revenue"].tail(N).to_numpy(dtype=float))
    mean_rev = float(monthly["revenue"].tail(N).mean())
    slope_norm = 0.0 if mean_rev <= 0 else float(slope / mean_rev)

    severity = _clip01(abs(slope_norm) * 8)
    if severity >= 0.25:
        direction = "increasing" if slope > 0 else "decreasing"
        conf_score = _clip01(0.65 + min(0.25, abs(slope_norm) * 2))

        out["signals"].append({
            "signal_type": "REVENUE_TRAJECTORY",
            "severity": severity_label(severity),
            "confidence": confidence_label(conf_score),
            "direction": direction,
            "summary": (
                f"Revenue increased from {out.get('window_start_month')} "
                f"to {out.get('window_end_month')}."
                if direction == "increasing"
                else
                f"Revenue decreased from {out.get('window_start_month')} "
                f"to {out.get('window_end_month')}."
            ),
            "recommended_action": "Validate completeness and check for structural changes in platform or territory mix.",
            "evidence": {
                "months_analyzed": N,
                "window": out.get("window_label"),
                "slope_normalized": slope_norm,
            },
        })

    # ---------- Platform level behavior ----------
    if "platform_org" in d.columns:
        platform_grouped = (
            d.groupby(["platform_org", "period_month"])[amount_col]
            .sum()
            .reset_index()
        )

        for platform, sub in platform_grouped.groupby("platform_org"):
            sub = sub.sort_values("period_month")
            if len(sub) < 4:
                continue

            series = sub[amount_col].to_numpy(dtype=float)
            behavior = classify_behavior(series)

            out["platform_behavior"].append({
                "platform": platform,
                **behavior,
            })
    # ---------- Territory behavior + disappearance ----------
    if "territory" in d.columns:
        territory_grouped = (
            d.groupby(["territory", "period_month"])[amount_col]
            .sum()
            .reset_index()
        )

        # portfolio total for relative materiality
        portfolio_total = float(monthly["revenue"].sum())
        last_month = months_list[-1] if months_list else None  # strings (you casted earlier)

        # Thresholds (tweak later)
        ABS_MATERIAL_USD = 50_000.0
        REL_MATERIAL_SHARE = 0.10
        EPS = 1e-9

        # We will align each territory to the full window months_list
        for territory, sub in territory_grouped.groupby("territory"):
            sub = sub.sort_values("period_month").copy()
            sub["period_month"] = sub["period_month"].astype(str)

            # Build aligned series across the full window (missing months => 0)
            month_to_val = dict(zip(sub["period_month"].tolist(), sub[amount_col].astype(float).tolist()))
            aligned = [float(month_to_val.get(m, 0.0)) for m in months_list]

            # Behavior (optional watchlist)
            if len(aligned) >= 4:
                behavior = classify_behavior(np.array(aligned, dtype=float))
                out.setdefault("territory_behavior", []).append({
                    "territory": str(territory),
                    **behavior,
                })

            # Disappearance detection (signal)
            if len(aligned) < 4 or not last_month:
                continue

            last_val = float(aligned[-1])
            prior_vals = np.array(aligned[:-1], dtype=float)

            prior_total = float(np.nansum(prior_vals))
            prior_mean = float(np.nanmean(prior_vals)) if len(prior_vals) else 0.0
            prior_nonzero_months = int(np.sum(prior_vals > EPS))

            share_of_portfolio = (prior_total / portfolio_total) if portfolio_total > 0 else 0.0

            is_material = (prior_total >= ABS_MATERIAL_USD) or (share_of_portfolio >= REL_MATERIAL_SHARE)

            # Reduce false positives:
            # - must have appeared in at least 2 prior months
            # - and now last month is ~0
            disappeared = (last_val <= EPS) and (prior_total > EPS) and (prior_nonzero_months >= 2)

            if is_material and disappeared and last_month:
                share_pct = round(float(share_of_portfolio) * 100, 1)

                out["signals"].append({
                    "signal_type": "TERRITORY_DISAPPEARANCE",
                    "severity": "High",
                    "confidence": "High",
                    "direction": "disappeared",
                    "summary": (
                        f"{territory} revenue dropped to $0 in {last_month} "
                        f"after averaging ${prior_mean:,.0f} per month in prior periods. "
                        f"This territory represented approximately {share_pct}% of portfolio revenue prior to disappearance."
                    ),
                    "recommended_action": (
                        "Confirm whether this is a reporting gap, a distribution/rights change, "
                        "or a territory mapping issue. Verify the territory appears in source statements "
                        "and check whether the platform export was filtered."
                    ),
                    "evidence": {
                        "territory": str(territory),
                        "last_month": str(last_month),
                        "last_month_revenue": float(last_val),
                        "prior_months_total": float(prior_total),
                        "prior_months_avg": float(prior_mean),
                        "prior_share_of_portfolio": round(float(share_of_portfolio), 4),
                        "window": out.get("window_label"),
                    },
                })

    # ---------- Final return (JSON-safe) ----------
    return {
        "window_months": int(out.get("window_months", 0)),

        # ✅ add these so the report + quick check can display the window
        "window_start_month": out.get("window_start_month"),
        "window_end_month": out.get("window_end_month"),
        "window_label": out.get("window_label"),

        "period_range": out.get("period_range"),
        "monthly_revenue": out.get("monthly_revenue", []),
        "portfolio_behavior": out.get("portfolio_behavior"),
        "platform_behavior": out.get("platform_behavior", []),
        "territory_behavior": out.get("territory_behavior", []),
        "signals": out.get("signals", []),
    }

# ============================================================
# COMPOSITE FINANCIAL EXPOSURE SCORING
# ============================================================

def compute_financial_exposure(report: Dict[str, Any]) -> Dict[str, Any]:
    """
    Produces a composite financial exposure rating based on:
    - Structural findings
    - Trend intelligence
    - Revenue concentration
    """

    score = 0.0
    drivers = []

    # -------------------------
    # 1) Structural Findings
    # -------------------------
    findings = report.get("findings", []) or []
    for f in findings:
        sev = f.get("severity")
        category = f.get("category", "")

        if sev == "High":
            score += 2.0
            drivers.append(category)
        elif sev == "Med":
            score += 1.0

    # -------------------------
    # 2) Trend Intelligence
    # -------------------------
    trend = report.get("trend_intelligence", {}) or {}
    signals = trend.get("signals", []) or []

    for s in signals:
        direction = s.get("direction")
        sig_type = s.get("signal_type")

        if sig_type == "PORTFOLIO_BEHAVIOR" and direction in {"declining", "volatile"}:
            score += 2.0
            drivers.append("Unstable portfolio revenue behavior")

        if sig_type == "REVENUE_TRAJECTORY" and direction == "decreasing":
            score += 1.5
            drivers.append("Revenue trending downward")

    # -------------------------
    # 3) Concentration Risk
    # -------------------------
    for f in findings:
        if f.get("category") == "Revenue concentration" and f.get("severity") == "High":
            score += 2.0
            drivers.append("High revenue concentration")

    # -------------------------
    # Normalize Score
    # -------------------------
    if score >= 6:
        label = "Critical"
    elif score >= 4:
        label = "Elevated"
    elif score >= 2:
        label = "Moderate"
    else:
        label = "Low"

    # Meaning descriptions
    meaning_map = {
        "Low": "Revenue appears structurally stable with no significant behavioral or concentration risks detected.",
        "Moderate": "Revenue behavior shows instability or early decline signals. Investigation is recommended, but no critical structural failures were detected.",
        "Elevated": "Multiple behavioral or structural risk signals detected. Revenue may be materially exposed to concentration, decline, or reporting gaps.",
        "Critical": "Significant financial risk indicators detected. Immediate investigation into reporting completeness and revenue stability is strongly recommended.",
    }

    return {
        "exposure_label": label,
        "exposure_score": round(score, 2),
        "meaning": meaning_map.get(label),
        "key_drivers": list(dict.fromkeys(drivers))[:5],
    }


# ============================================================
# ANALYZE + OUTPUT REPORT
# ============================================================

def analyze_dataframe(df: pd.DataFrame, load_meta: dict, verbose: bool = False) -> Dict[str, Any]:
    df = standardize_schema(df)
    statement_type = detect_statement_type(df)
    family_info = classify_statement_family(df, load_meta)

    if verbose:
        print("COLUMNS:", list(df.columns))

    if "period_start" in df.columns:
        df["period_start"] = coerce_date(df["period_start"])
    if "period_end" in df.columns:
        df["period_end"] = coerce_date(df["period_end"])

    amount_col = detect_amount_column(df)
    df[amount_col] = coerce_numeric(df[amount_col])
    if verbose:
        print("Detected amount column:", amount_col)

    df = ensure_period_month(df)

    findings: List[Finding] = []

    if statement_type == "summary":
        findings += check_negative_amounts(df, amount_col)
        findings += check_duplicates_summary(df, amount_col)
        findings += check_suspicious_territories(df)
        findings += check_missing_periods(df)
        findings += check_platform_revenue_outliers(df, amount_col)
        findings += check_revenue_concentration(df, amount_col)
        findings += check_missing_expected_platforms(df, amount_col)

    elif statement_type == "line_item":
        findings += check_missing_line_item_ids(df)
        findings += check_negative_amounts(df, amount_col)
        findings += check_zero_or_blank_amounts(df, amount_col)
        findings += check_rate_math(df, amount_col)

    else:
        findings.append(
            Finding(
                id=stable_id("ambiguous_statement"),
                category="Statement type unclear",
                severity="Low",
                confidence=0.6,
                summary="This file does not clearly match a summary or line-item royalty statement format.",
                recommended_action="Export a detailed (line-item) royalty report if available, ideally including track/ISRC and earnings columns.",
                evidence=[],
            )
        )

    total_rows = int(len(df))
    total_amount = float(df[amount_col].fillna(0).sum())

    coverage_overview = build_royalty_stream_coverage(df, statement_type)
    estimated_contextual_ranges = build_contextual_range_estimates(total_amount)
    prioritized_opportunities = build_prioritized_opportunities(estimated_contextual_ranges)

    top_revenue_drivers: List[Dict[str, Any]] = []
    if statement_type == "line_item":
        top_revenue_drivers = build_top_revenue_drivers(df, amount_col, top_n=10)

    trend_intelligence = compute_trend_intelligence(df, amount_col) or {"window_months": 0, "signals": []}
    revenue_risk_level = compute_revenue_risk_level(trend_intelligence or {})

    disclosures = [
        "Coverage Overview indicates which royalty streams are represented in the uploaded dataset vs. commonly reported elsewhere.",
        "Estimated Contextual Ranges are provided for prioritization only and do not represent statements of entitlement, unpaid royalties, or guarantees of recovery.",
        "Actual results depend on ownership, splits, registrations, reporting timelines, territory, and statement type (summary vs line-item).",
    ]

    report = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "input": load_meta,
        "summary": {
            "statement_type": statement_type,
            "statement_family": family_info.get("family"),
            "family_confidence": family_info.get("confidence"),
            "total_rows": total_rows,
            "amount_column": amount_col,
            "total_amount": total_amount,
            "findings_count": len(findings),
            "top_categories": (
                pd.Series([f.category for f in findings]).value_counts().to_dict()
                if findings else {}
            ),
        },
        "statement_family": family_info,
        "trend_intelligence": trend_intelligence,
        "revenue_risk_level": revenue_risk_level,
        "coverage_overview": coverage_overview,
        "estimated_contextual_ranges": estimated_contextual_ranges,
        "prioritized_opportunities": prioritized_opportunities,
        "top_revenue_drivers": top_revenue_drivers,
        "disclosures": disclosures,
        "findings": [asdict(f) for f in findings],
    }

    # keep your existing composite exposure score
    report["financial_exposure"] = compute_financial_exposure(report)

    return report

def build_package_coverage_map(unit_reports: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Summarizes which statement families are present across the uploaded package,
    which are commonly expected but missing, and what to request next.
    """
    # Count families
    fam_counts: Dict[str, int] = {}
    windows: Dict[str, int] = {}

    for r in unit_reports or []:
        summ = r.get("summary", {}) or {}
        fam = summ.get("statement_family") or "UNKNOWN"
        fam_counts[fam] = fam_counts.get(fam, 0) + 1

        trend = r.get("trend_intelligence", {}) or {}
        window = trend.get("window_label") or None
        if window:
            windows[window] = windows.get(window, 0) + 1

    families_present = {k: v for k, v in fam_counts.items() if k != "UNKNOWN" and v > 0}

    # What a “complete” manager package typically includes
    expected = [
        "DISTRIBUTOR_LABEL_MASTER",
        "MLC_MECHANICAL",
        "PRO_PERFORMANCE",
        "NEIGHBORING_RIGHTS",
    ]

    missing = [f for f in expected if f not in families_present]

    # Recommendation text per missing family
    rec_map = {
        "MLC_MECHANICAL": "Upload the MLC mechanical statement/export for the same reporting window (US mechanicals).",
        "PRO_PERFORMANCE": "Upload PRO distributions (ASCAP/BMI/SESAC/PRS etc.) for the same window (public performance).",
        "NEIGHBORING_RIGHTS": "Upload neighboring rights statements (SoundExchange/PPL/GVL etc.), especially if radio/digital performance is expected.",
        "DISTRIBUTOR_LABEL_MASTER": "Upload distributor/label royalty statements (DSP/platform earnings) for the same window.",
    }

    # Optional: window-aware recommendations
    window_note = None
    if windows:
        # pick the most common window label in the package
        common_window = sorted(windows.items(), key=lambda x: x[1], reverse=True)[0][0]
        window_note = f"Most common reporting window in this package: {common_window}"

    recommended_next_uploads = [rec_map[m] for m in missing if m in rec_map]

    notes = []
    if not families_present:
        notes.append("No statement families could be confidently classified; consider adding name hints (e.g., 'MLC', 'PRS', 'SoundExchange') to files/tabs.")
    if window_note:
        notes.append(window_note)

    return {
        "families_present": families_present,
        "missing_families": missing,
        "recommended_next_uploads": recommended_next_uploads,
        "notes": notes,
    }

def build_package_coverage_finding(package_coverage: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    Convert package_coverage gaps into a Finding-like dict so it appears in each unit report + DOCX.
    """
    coverage = package_coverage or {}
    missing = coverage.get("missing_families", []) or []
    recs = coverage.get("recommended_next_uploads", []) or []

    if not missing:
        return None

    # Severity heuristic
    if "DISTRIBUTOR_LABEL_MASTER" in missing:
        severity = "High"
        confidence = 0.9
    elif len(missing) >= 3:
        severity = "High"
        confidence = 0.85
    else:
        severity = "Med"
        confidence = 0.8

    summary = (
        "Package coverage gaps detected: missing "
        + ", ".join(missing)
        + "."
    )

    recommended_action = " ".join(recs) if recs else "Upload the missing statement families for the same reporting window."

    finding = {
        "id": stable_id("package_coverage_gaps", ",".join(missing)),
        "category": "Package coverage gaps",
        "severity": severity,
        "confidence": float(confidence),
        "summary": summary,
        "recommended_action": recommended_action,
        "evidence": [],  # package-level, not row-level
    }
    return finding
def compute_cross_stream_diagnostics(package_report: Dict[str, Any]) -> Dict[str, Any]:
    """
    Package-level diagnostic layer: cross-references unit reports, infers implied streams,
    and flags misalignment / under-indexing across streams.
    """

    rollup = package_report.get("workbook_rollup", {}) or {}
    coverage = package_report.get("package_coverage", {}) or {}
    unit_reports = package_report.get("unit_reports", []) or []
    units = rollup.get("units", []) or []

    present = coverage.get("families_present", {}) or {}
    missing = set(coverage.get("missing_families", []) or [])

    # ----------------------------------------------------
    # Revenue by statement family
    # ----------------------------------------------------
    revenue_by_family = {}

    for r in unit_reports:
        fam = (r.get("summary", {}) or {}).get("statement_family") or "UNKNOWN"
        amt = float((r.get("summary", {}) or {}).get("total_amount", 0.0) or 0.0)
        revenue_by_family[fam] = revenue_by_family.get(fam, 0.0) + amt

    # ----------------------------------------------------
    # Diagnostic containers
    # ----------------------------------------------------
    implied = []
    signals = []
    actions = []

    master_rev = revenue_by_family.get("DISTRIBUTOR_LABEL_MASTER", 0.0)

    # ----------------------------------------------------
    # 1) MLC implied by DSP master revenue
    # ----------------------------------------------------
    if master_rev > 0 and "MLC_MECHANICAL" in missing:
        implied.append({
            "stream": "MLC_MECHANICAL",
            "probability": 0.75,
            "reason": "DSP master revenue present; US mechanicals typically exist if compositions are registered/claimed."
        })

        signals.append({
            "signal_type": "MISSING_BUT_IMPLIED",
            "severity": "High" if master_rev >= 250000 else "Medium",
            "confidence": "High",
            "summary": "Mechanical royalties (MLC) appear missing but are strongly implied by DSP master revenue.",
            "recommended_action": "Upload the MLC statement/export for the same months; confirm works are registered/claimed and matched."
        })

        actions.append(
            "Request/download MLC details for the same reporting window; verify songwriter/publisher registrations and matches."
        )

    # ----------------------------------------------------
    # 2) PRO implied
    # ----------------------------------------------------
    if master_rev > 0 and "PRO_PERFORMANCE" in missing:
        implied.append({
            "stream": "PRO_PERFORMANCE",
            "probability": 0.60,
            "reason": "DSP revenue implies public performance depending on territory and registrations."
        })

        signals.append({
            "signal_type": "MISSING_BUT_IMPLIED",
            "severity": "Medium",
            "confidence": "Medium",
            "summary": "PRO performance statements may be missing for at least one key territory.",
            "recommended_action": "Upload ASCAP/BMI/SESAC/PRS (or relevant PRO) distributions for the same window; verify work registrations."
        })

        actions.append(
            "Pull PRO distributions for the same window; confirm registrations/CAE/IPI and work-title matching."
        )

    # ----------------------------------------------------
    # 3) Neighboring rights implied
    # ----------------------------------------------------
    if master_rev > 0 and "NEIGHBORING_RIGHTS" in missing:
        implied.append({
            "stream": "NEIGHBORING_RIGHTS",
            "probability": 0.45,
            "reason": "If recordings have radio/digital public performance exposure, neighboring rights can exist."
        })

        signals.append({
            "signal_type": "MISSING_BUT_IMPLIED",
            "severity": "Low",
            "confidence": "Medium",
            "summary": "Neighboring rights statements are not present; may be relevant depending on radio/digital performance exposure.",
            "recommended_action": "If radio/digital performance is expected, upload SoundExchange/PPL/GVL statements (same window)."
        })

    # ----------------------------------------------------
    # 4) Under-index check (PRO vs Master)
    # ----------------------------------------------------
    pro_rev = revenue_by_family.get("PRO_PERFORMANCE", 0.0)

    if master_rev > 0 and pro_rev > 0:
        ratio = pro_rev / master_rev

        if ratio < 0.01:
            signals.append({
                "signal_type": "UNDER_INDEX",
                "severity": "Medium",
                "confidence": "Medium",
                "summary": "Performance revenue appears low relative to master revenue; could indicate incomplete registrations, territory gaps, or timing mismatches.",
                "recommended_action": "Confirm works are registered correctly at PRO(s); check territory coverage and statement window alignment."
            })

    # ----------------------------------------------------
    # Final structured output
    # ----------------------------------------------------
    return {
        "total_units": len(units),
        "present_families": list(present.keys()),
        "missing_families": sorted(list(missing)),
        "revenue_by_family": revenue_by_family,
        "implied_streams": implied,
        "signals": signals,
        "recommended_actions": actions[:6],
    }



def analyze_package(input_path: str, verbose: bool = False) -> Dict[str, Any]:
    pkg = load_statement_package(input_path)
    package_meta = pkg["package_meta"]
    units = pkg["units"]

    unit_reports = []
    for u in units:
        rep = analyze_dataframe(u["df"], u["meta"], verbose=verbose)
        unit_reports.append(rep)

    package_coverage = build_package_coverage_map(unit_reports)

    # --- Stage 4C: Inject package coverage gaps into each unit report as a Finding ---
    pkg_finding = build_package_coverage_finding(package_coverage)

    if pkg_finding:
        for r in unit_reports:
            r.setdefault("findings", [])
            # Put it first so it appears prominently in docs
            r["findings"].insert(0, pkg_finding)

            # Keep summary counts consistent
            summ = r.get("summary", {}) or {}
            summ["findings_count"] = len(r["findings"])

            cats = [f.get("category") for f in r["findings"] if f.get("category")]
            summ["top_categories"] = pd.Series(cats).value_counts().to_dict() if cats else {}
            r["summary"] = summ

    # Keep a simple pointer too (fine to keep)
    for r in unit_reports:
        r["package_coverage"] = package_coverage

    # Rollup summary (MUST exist before cross-stream uses it)
    rollup = {
        "unit_count": len(unit_reports),
        "units": [
            {
                "unit_name": (r.get("input", {}) or {}).get("sheet")
                             or (r.get("input", {}) or {}).get("zip_member")
                             or Path(((r.get("input", {}) or {}).get("source_path") or "")).name,
                "statement_type": (r.get("summary", {}) or {}).get("statement_type"),
                "total_rows": (r.get("summary", {}) or {}).get("total_rows"),
                "total_amount": (r.get("summary", {}) or {}).get("total_amount"),
                "findings_count": (r.get("summary", {}) or {}).get("findings_count"),
                "trend_signals": len(((r.get("trend_intelligence", {}) or {}).get("signals", []) or [])),
            }
            for r in unit_reports
        ],
    }

    # Build a package_report object first (real-world: everything in one structure)
    package_report: Dict[str, Any] = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "package_meta": package_meta,
        "workbook_rollup": rollup,
        "unit_reports": unit_reports,
        "package_coverage": package_coverage,
    }

    # Cross-stream diagnostics ONCE, using the complete package context
    cross_stream = compute_cross_stream_diagnostics(package_report)
    package_report["cross_stream_diagnostics"] = cross_stream

    # Optional: also attach to each unit so unit DOCX can reference it
    for r in unit_reports:
        r["cross_stream_diagnostics"] = cross_stream

    return package_report



def write_report_json(report: Dict[str, Any], out_path: str) -> None:
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)


# ============================================================
# EXECUTIVE DOCX REPORT GENERATOR
# ============================================================

def build_risk_summary(report: Dict[str, Any]) -> str:
    findings = report.get("findings", []) or []

    rr = report.get("revenue_risk_level", {}) or {}
    level = (rr.get("revenue_risk_level") or "").strip()

    if not findings:
        if level in {"High", "Moderate"}:
            return (
                "No major reporting irregularities were detected in this upload, "
                "but revenue behavior signals indicate elevated risk."
            )
        return "No major reporting irregularities were detected in this upload."

    cats = []
    for f in findings:
        if (f.get("severity") == "High") and (f.get("category") in {
            "Revenue concentration",
            "Coverage gap risk",
            "Territory concentration risk",
            "Missing reporting periods",
            "Platform revenue anomalies",
            "Package coverage gaps",
        }):
            cats.append(f.get("category"))

    cats = list(dict.fromkeys([c for c in cats if c]))
    if not cats:
        return "No major reporting irregularities were detected in this upload."

    mapping = {
        "Revenue concentration": "High concentration",
        "Coverage gap risk": "Possible missing platform coverage",
        "Territory concentration risk": "Possible missing territory scope",
        "Missing reporting periods": "Missing reporting periods",
        "Platform revenue anomalies": "Platform revenue anomalies",
        "Package coverage gaps": "Missing statement coverage (package-level)",
    }


    labels = [mapping.get(c, c) for c in cats]
    return "Risk Summary: " + "; ".join(labels) + "."

# ============================================================
# EXECUTIVE DOCX (PER UNIT)
# ============================================================

def generate_executive_docx(report: Dict[str, Any]) -> str:
    """
    Diagnostic-style executive DOCX (BM-friendly).
    Uses BOTH unit-level signals and package-level intelligence (coverage + cross-stream diagnostics).
    """
    from pathlib import Path
    from datetime import datetime
    from docx import Document
    from docx.shared import Pt, Inches
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    from docx.oxml import OxmlElement
    from docx.oxml.ns import qn

    doc = Document()

    # ---------- Helpers (inside function) ----------
    def set_cell_shading(cell, fill="F3F4F6"):
        tcPr = cell._tc.get_or_add_tcPr()
        shd = OxmlElement("w:shd")
        shd.set(qn("w:val"), "clear")
        shd.set(qn("w:color"), "auto")
        shd.set(qn("w:fill"), fill)
        tcPr.append(shd)

    def bold_cell(cell, size=10):
        for p in cell.paragraphs:
            for run in p.runs:
                run.bold = True
                run.font.size = Pt(size)

    def style_table(table, header_fill="E3E7EE", zebra=True):
        table.style = "Table Grid"
        table.allow_autofit = True

        hdr_cells = table.rows[0].cells
        for c in hdr_cells:
            set_cell_shading(c, header_fill)
            bold_cell(c, size=10)

        # zebra striping (optional)
        if zebra and len(table.rows) > 1:
            for ridx, r in enumerate(table.rows[1:], start=1):
                if ridx % 2 == 0:
                    for c in r.cells:
                        set_cell_shading(c, "F6F7F9")

    def add_divider():
        p = doc.add_paragraph("—" * 28)
        p.paragraph_format.space_before = Pt(6)
        p.paragraph_format.space_after = Pt(10)

    def add_kv(label: str, value: str):
        p = doc.add_paragraph()
        r = p.add_run(f"{label}: ")
        r.bold = True
        p.add_run(str(value))

    def bullet(text: str):
        doc.add_paragraph(str(text), style="List Bullet")

    # ---------- Metadata ----------
    local_now = datetime.now()
    display_date = local_now.strftime("%B %d, %Y")
    filename_date = local_now.strftime("%Y-%m")

    inp = report.get("input", {}) or {}

    source_path = inp.get("source_path", "") or ""
    base_slug = Path(source_path).stem.replace(" ", "_") if source_path else "Upload"

    sheet = inp.get("sheet")
    zip_member = inp.get("zip_member")
    artist_key = inp.get("artist_key")

    # Unit name (for title)
    unit_name = (
        inp.get("artist_name")
        or inp.get("artist_key")
        or inp.get("sheet")
        or inp.get("zip_member")
        or Path(inp.get("source_path", "")).stem
        or "Unknown_Unit"
    )

    # Filename slug (avoid collisions)
    parts = [base_slug]
    if zip_member:
        parts.append(Path(zip_member).stem.replace(" ", "_"))
    if sheet:
        parts.append(str(sheet).replace(" ", "_"))
    if artist_key and str(artist_key) not in parts:
        parts.append(str(artist_key).replace(" ", "_"))
    unit_slug = "_".join([p for p in parts if p]) or "Unit"

    filename = f"RI_Report_{unit_slug}_{filename_date}.docx"

    # Package intelligence (cross-reference)
    pkg_ctx = report.get("package_context", {}) or {}
    pkg_cov = pkg_ctx.get("package_coverage", {}) or {}
    cross = pkg_ctx.get("cross_stream_diagnostics", {}) or {}

    # ---------- Title ----------
    title = doc.add_heading("Royalty Intelligence — Diagnostic Report", level=1)
    title.alignment = WD_ALIGN_PARAGRAPH.CENTER

    sub = doc.add_paragraph(str(unit_name))
    sub.alignment = WD_ALIGN_PARAGRAPH.CENTER

    date_para = doc.add_paragraph(f"Report Date: {display_date}")
    date_para.alignment = WD_ALIGN_PARAGRAPH.CENTER

    doc.add_paragraph("")

    # ---------- 1) Executive Summary ----------
    doc.add_heading("1) Executive Summary", level=2)

    summ = report.get("summary", {}) or {}
    statement_type = summ.get("statement_type", "Unknown")
    statement_family = summ.get("statement_family", "UNKNOWN")
    total_amount = float(summ.get("total_amount", 0.0) or 0.0)
    total_rows = summ.get("total_rows", None)

    add_kv("Statement type", statement_type)
    add_kv("Statement family", statement_family)
    add_kv("Rows analyzed", total_rows if total_rows is not None else "Unknown")
    add_kv("Revenue in this upload", f"${total_amount:,.2f}")

    # Package coverage cross-reference
    present = pkg_cov.get("families_present", {}) or {}
    missing = pkg_cov.get("missing_families", []) or []
    if present:
        add_kv("Coverage present in package", ", ".join([f"{k}({v})" for k, v in present.items()]))
    if missing:
        add_kv("Likely missing streams (package)", ", ".join(missing))

    # Top cross-stream signals
    cs_signals = cross.get("signals", []) or []
    if cs_signals:
        doc.add_paragraph("Package-level intelligence flagged:")
        for s in cs_signals[:2]:
            bullet(s.get("summary") or "")

    add_divider()

    # ---------- 2) Data Summary ----------
    doc.add_heading("2) Data Summary", level=2)

    findings = report.get("findings", []) or []
    add_kv("Flags detected", str(len(findings)))

    # If you have drivers, show a small evidence table (BM-friendly)
    drivers = report.get("top_revenue_drivers", []) or []
    if drivers:
        doc.add_paragraph("Top revenue drivers (from this upload):")

        t = doc.add_table(rows=1, cols=4)
        style_table(t)
        hdr = t.rows[0].cells
        hdr[0].text = "Track"
        hdr[1].text = "Platform"
        hdr[2].text = "Territory"
        hdr[3].text = "Amount"

        for d in drivers[:10]:
            row = t.add_row().cells
            row[0].text = str(d.get("track_title", "") or "")[:60]
            row[1].text = str(d.get("platform_org", "") or "")[:30]
            row[2].text = str(d.get("territory", "") or "")[:20]
            row[3].text = f"${float(d.get('amount_usd', 0.0) or 0.0):,.2f}"

    add_divider()

    # ---------- 3) Methodology ----------
    doc.add_heading("3) Methodology (How to read this)", level=2)
    doc.add_paragraph(
        "This report is a diagnostic prioritization layer. It flags patterns and likely gaps based on the "
        "uploaded statements, their structure, and package-level cross-referencing. "
        "Estimated ranges (when shown) are conservative and depend on registrations, splits, usage mix, "
        "reporting windows, and contract terms."
    )

    add_divider()

    # ---------- 4) Ranked Recovery Opportunities ----------
    doc.add_heading("4) Ranked Recovery Opportunities", level=2)

    # Use unit prioritization if present; otherwise use implied package streams
    ranges = report.get("estimated_contextual_ranges", []) or []
    prios = report.get("prioritized_opportunities", []) or []
    prio_map = {p.get("stream"): p for p in prios if p.get("stream")}

    implied = cross.get("implied_streams", []) or []
    opportunities = []

    if ranges:
        for item in ranges:
            stream = item.get("stream")
            if not stream:
                continue
            low = float((item.get("estimated_range_usd", {}) or {}).get("low", 0.0) or 0.0)
            high = float((item.get("estimated_range_usd", {}) or {}).get("high", 0.0) or 0.0)
            conf = item.get("confidence", "Unknown")
            rank = (prio_map.get(stream, {}) or {}).get("priority_rank", 99)
            opportunities.append({
                "rank": rank,
                "stream": stream,
                "range": f"${low:,.0f} – ${high:,.0f}" if (low or high) else "Range not available",
                "confidence": conf,
            })
        opportunities.sort(key=lambda x: x["rank"])
    else:
        for idx, imp in enumerate(implied, start=1):
            opportunities.append({
                "rank": idx,
                "stream": imp.get("stream"),
                "range": "Depends on registrations/splits",
                "confidence": f"Implied (p={imp.get('probability')})",
            })

    if not opportunities:
        doc.add_paragraph("No ranked opportunities could be generated from the current data.")
    else:
        for i, opp in enumerate(opportunities[:4], start=1):
            doc.add_paragraph(f"{i}. {opp.get('stream')}", style=None)
            bullet(f"Estimated range: {opp.get('range')}")
            bullet(f"Confidence: {opp.get('confidence')}")

            # Try to attach a relevant recommended action from cross-stream signals
            rec = None
            for s in cs_signals:
                if (s.get("recommended_action") or "") and (opp.get("stream") or ""):
                    if opp["stream"] in (s.get("summary") or "") or opp["stream"] in (s.get("recommended_action") or ""):
                        rec = s.get("recommended_action")
                        break

            if rec:
                p = doc.add_paragraph()
                r = p.add_run("Recommended next step: ")
                r.bold = True
                p.add_run(str(rec))

            add_divider()

    # ---------- 5) Manager Action Plan ----------
    doc.add_heading("5) Manager Action Plan (Next 7–14 days)", level=2)

    actions = cross.get("recommended_actions", []) or []
    if actions:
        for a in actions[:6]:
            bullet(a)
    else:
        bullet("Upload missing statements for the same reporting window (MLC / PRO / Neighboring Rights).")
        bullet("Verify work registrations + matching (titles, writers, IPI/CAE, ISRC/ISWC).")
        bullet("Re-run diagnostics using line-item exports for deeper validation.")

    add_divider()

    # ---------- Flags Detected ----------
    doc.add_heading("Flags Detected (From This Upload)", level=2)

    if findings:
        for f in findings[:12]:
            bullet(f.get("summary") or f.get("category") or "Finding")
    else:
        bullet("No anomalies were detected by the current rule set for this upload.")

    doc.save(filename)
    return filename



# ============================================================
# MASTER PACKAGE DOCX
# ============================================================

def generate_master_package_docx(package_report: Dict[str, Any]) -> str:
    """
    Generates a single executive summary DOCX for the entire uploaded package,
    including estimated opportunity ranges aggregated across units.
    """
    from docx import Document
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    from pathlib import Path
    from datetime import datetime

    package_meta = package_report.get("package_meta", {}) or {}
    rollup = package_report.get("workbook_rollup", {}) or {}
    coverage = package_report.get("package_coverage", {}) or {}
    unit_reports = package_report.get("unit_reports", []) or []
    units = rollup.get("units", []) or []

    cross = package_report.get("cross_stream_diagnostics", {}) or {}
    cross_signals = cross.get("signals", []) or []

    doc = Document()

    # -------------------------
    # Helpers
    # -------------------------
    def money(x: float) -> str:
        try:
            return f"${float(x):,.0f}"
        except Exception:
            return "$0"

    # -------------------------
    # Coverage vars (define ONCE)
    # -------------------------
    present = coverage.get("families_present", {}) or {}
    missing = coverage.get("missing_families", []) or []
    recs = coverage.get("recommended_next_uploads", []) or []

    # -------------------------
    # Aggregate estimated opportunity ranges across units (define ONCE)
    # -------------------------
    # unit format:
    # r["estimated_contextual_ranges"] = [{"stream": "...", "estimated_range_usd": {"low":..., "high":...}, "confidence": "..."}]
    agg: Dict[str, Dict[str, Any]] = {}  # stream -> {"low": sum, "high": sum, "conf_counts": {...}}

    for r in unit_reports:
        for item in (r.get("estimated_contextual_ranges", []) or []):
            stream = item.get("stream") or "Unknown"
            rng = item.get("estimated_range_usd", {}) or {}
            low = float(rng.get("low", 0) or 0)
            high = float(rng.get("high", 0) or 0)
            conf = str(item.get("confidence") or "Unknown").strip()

            if stream not in agg:
                agg[stream] = {"low": 0.0, "high": 0.0, "conf_counts": {}}

            agg[stream]["low"] += low
            agg[stream]["high"] += high
            agg[stream]["conf_counts"][conf] = agg[stream]["conf_counts"].get(conf, 0) + 1

    # -------------------------
    # Title + Header
    # -------------------------
    title = doc.add_heading("Royalty Intelligence — Package Executive Report", level=1)
    title.alignment = WD_ALIGN_PARAGRAPH.LEFT

    source_name = package_meta.get("source_path", "Package")
    doc.add_paragraph(f"Source: {source_name}")
    doc.add_paragraph(f"Units analyzed: {rollup.get('unit_count', len(units))}")

    # Total revenue across units
    total_revenue = float(sum(float(u.get("total_amount", 0) or 0) for u in units))
    doc.add_paragraph(f"Total reported revenue (all units): {money(total_revenue)}")
    doc.add_paragraph("")

    # =====================================================
    # EXECUTIVE SNAPSHOT
    # =====================================================
    doc.add_heading("Executive Snapshot", level=2)

    total_low = float(sum(float(v.get("low", 0) or 0) for v in agg.values()))
    total_high = float(sum(float(v.get("high", 0) or 0) for v in agg.values()))

    # Structural risk scoring
    risk_score = 0
    if missing:
        risk_score += 2

    for s in (cross_signals or []):
        sev = str(s.get("severity") or "Medium").strip()
        if sev == "High":
            risk_score += 2
        elif sev == "Medium":
            risk_score += 1

    if risk_score >= 5:
        structural_risk = "High"
    elif risk_score >= 2:
        structural_risk = "Medium"
    else:
        structural_risk = "Low"

    # Highest implied stream (by agg high)
    top_stream = None
    if agg:
        ranked_streams = sorted(agg.items(), key=lambda kv: float(kv[1].get("high", 0) or 0), reverse=True)
        top_stream = ranked_streams[0][0]

    # Immediate action (strongest severity signal)
    immediate_action = None
    if cross_signals:
        sev_rank = {"High": 3, "Medium": 2, "Low": 1}
        strongest = sorted(
            cross_signals,
            key=lambda x: sev_rank.get(str(x.get("severity") or "Medium").strip(), 2),
            reverse=True
        )[0]
        immediate_action = strongest.get("recommended_action")

    doc.add_paragraph(f"• Implied Missing Revenue Range: {money(total_low)} – {money(total_high)}")
    doc.add_paragraph(f"• Structural Risk Level: {structural_risk}")

    if top_stream:
        doc.add_paragraph(f"• Highest Priority Stream: {top_stream}")
    if immediate_action:
        doc.add_paragraph(f"• Immediate Action: {immediate_action}")

    doc.add_paragraph("")

    # -------------------------
    # Coverage Summary
    # -------------------------
    doc.add_heading("Statement Coverage", level=2)

    if present:
        doc.add_paragraph("Coverage present:")
        for k, v in present.items():
            doc.add_paragraph(f"- {k} ({v} units)", style="List Bullet")

    if missing:
        doc.add_paragraph("Missing coverage:")
        for m in missing:
            doc.add_paragraph(f"- {m}", style="List Bullet")

    if recs:
        doc.add_paragraph("Recommended next uploads:")
        for r in recs:
            doc.add_paragraph(f"- {r}", style="List Bullet")

    doc.add_paragraph("")

    # -------------------------
    # Package Estimated Opportunity Ranges
    # -------------------------
    doc.add_heading("Estimated Opportunity Ranges (Package)", level=2)

    if not agg:
        doc.add_paragraph(
            "No package-level ranges are available yet. "
            "This typically means unit reports did not include estimated_contextual_ranges."
        )
    else:
        ranked = sorted(agg.items(), key=lambda kv: float(kv[1].get("high", 0) or 0), reverse=True)

        for stream, v in ranked[:6]:
            conf_counts = v.get("conf_counts", {}) or {}
            conf_mode = (
                sorted(conf_counts.items(), key=lambda kv2: kv2[1], reverse=True)[0][0]
                if conf_counts else "Unknown"
            )

            doc.add_paragraph(
                f"- {stream}: {money(v['low'])} – {money(v['high'])} (Confidence: {conf_mode})",
                style="List Bullet",
            )

        doc.add_paragraph(
            "Note: Ranges are conservative and depend on registrations, splits, usage mix, reporting windows, and contract terms."
        )

    doc.add_paragraph("")

    # -------------------------
    # Top Revenue Units
    # -------------------------
    doc.add_heading("Top Revenue Units", level=2)

    top_units = sorted(units, key=lambda x: x.get("total_amount", 0) or 0, reverse=True)[:5]
    for u in top_units:
        doc.add_paragraph(
            f"- {u.get('unit_name')}: {money(u.get('total_amount', 0) or 0)}",
            style="List Bullet"
        )

    doc.add_paragraph("")

    # -------------------------
    # Cross-stream intelligence signals (if present)
    # -------------------------
    if cross_signals:
        doc.add_heading("Package-Level Intelligence Signals", level=2)

        for s in cross_signals[:6]:
            summ = str(s.get("summary") or "").strip()
            sev = str(s.get("severity") or "Medium").strip()
            conf = str(s.get("confidence") or "Medium").strip()
            rec = str(s.get("recommended_action") or "").strip()

            doc.add_paragraph(f"- {summ} (Impact: {sev} | Certainty: {conf})", style="List Bullet")
            if rec:
                doc.add_paragraph(f"  Start here: {rec}")

        doc.add_paragraph("")

    # -------------------------
    # High severity findings across units
    # -------------------------
    doc.add_heading("High Risk Flags (Across Units)", level=2)

    high_flags: List[str] = []
    for r in unit_reports:
        for f in (r.get("findings", []) or []):
            sev_val = f.get("severity")
            sev = str(sev_val).strip() if sev_val is not None else ""
            if sev == "High":
                high_flags.append(str(f.get("summary") or "").strip())

    high_flags = [x for x in dict.fromkeys(high_flags) if x][:8]

    if high_flags:
        for flag in high_flags:
            doc.add_paragraph(f"- {flag}", style="List Bullet")
    else:
        doc.add_paragraph("No high severity findings detected.")

    # -------------------------
    # Save
    # -------------------------
    filename_date = datetime.now().strftime("%Y-%m")
    base_slug = Path(str(source_name)).stem.replace(" ", "_")
    filename = f"RI_Master_Package_Report_{base_slug}_{filename_date}.docx"

    doc.save(filename)
    return filename




# ============================================================
# RUN
# ============================================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Royalty Intelligence - Local Demo Runner")
    parser.add_argument("input", help="Path to file (.csv/.xlsx/.zip)")
    parser.add_argument("--outdir", default="reports", help="Output directory for reports")
    parser.add_argument("--verbose", action="store_true", help="Print debug logs")
    args = parser.parse_args()

    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    # 1) Run package analysis (returns package_report)
    package_report = analyze_package(args.input, verbose=args.verbose)

    # 2) Ensure cross-stream diagnostics exist (analyze_package should already set it)
    cross_stream = package_report.get("cross_stream_diagnostics") or compute_cross_stream_diagnostics(package_report)
    package_report["cross_stream_diagnostics"] = cross_stream

    # 3) Attach package context to each unit report (so per-unit DOCX can reference it)
    unit_reports = package_report.get("unit_reports", []) or []
    package_coverage = package_report.get("package_coverage", {}) or {}

    for r in unit_reports:
        r["package_context"] = {
            "package_coverage": package_coverage,
            "cross_stream_diagnostics": cross_stream,
        }

    # 4) Generate per-unit DOCX files
    docx_files: List[str] = []
    master_docx: Optional[str] = None

    if not unit_reports:
        print("No analyzable units found in the uploaded file.")
    else:
        for r in unit_reports:
            docx_filename = generate_executive_docx(r)
            p = Path(docx_filename)

            # move into outdir
            try:
                p.replace(outdir / p.name)
                docx_filename = str(outdir / p.name)
            except Exception:
                pass

            docx_files.append(docx_filename)

        # 5) Generate master package DOCX
        master_docx = generate_master_package_docx(package_report)
        mp = Path(master_docx)
        try:
            mp.replace(outdir / mp.name)
            master_docx = str(outdir / mp.name)
        except Exception:
            pass

    # 6) Save JSON ONCE
    json_path = outdir / "royalty_intelligence_report.json"
    write_report_json(package_report, str(json_path))

    # 7) Print clean demo summary ONCE
    rollup = package_report.get("workbook_rollup", {}) or {}
    units = rollup.get("units", []) or []
    coverage = package_report.get("package_coverage", {}) or {}

    print("\n=== Royalty Intelligence Demo ===")
    print(f"Input: {args.input}")
    print(f"Units analyzed: {rollup.get('unit_count', len(units))}")

    top_units = sorted(units, key=lambda x: x.get("total_amount", 0) or 0, reverse=True)[:5]
    print("\nTop units by reported revenue:")
    for u in top_units:
        name = u.get("unit_name") or u.get("sheet") or u.get("zip_member") or "Unit"
        amt = float(u.get("total_amount", 0) or 0)
        print(f" - {name}: ${amt:,.2f}")

    present = coverage.get("families_present", {}) or {}
    missing = coverage.get("missing_families", []) or []

    if present:
        present_str = ", ".join([f"{k}({v})" for k, v in present.items()])
        print(f"\nCoverage present: {present_str}")

    if missing:
        print("Missing coverage:", ", ".join(missing))

    recs = coverage.get("recommended_next_uploads", []) or []
    if recs:
        print("\nRecommended next uploads:")
        for r in recs[:6]:
            print(f" - {r}")

    cs_signals = (cross_stream.get("signals") or [])[:6]
    if cs_signals:
        print("\nCross-stream diagnostic signals:")
        for s in cs_signals:
            sev = s.get("severity", "Unknown")
            summ = s.get("summary", "")
            print(f" - [{sev}] {summ}")

    if docx_files:
        print("\nUnit reports:")
        for p in docx_files:
            print(f" - {p}")

    print(f"\nMaster package report: {master_docx if master_docx else '(not generated)'}")
    print(f"JSON report: {json_path}\n")
